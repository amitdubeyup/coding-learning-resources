# AI Agent Interview Questions and Answers

This document contains a comprehensive set of interview questions and detailed answers based on AI agents built using FastAPI, LangChain/LangGraph, VectorDB, FAISS, PostgreSQL, Redis, and WebSocket. These questions are designed to help you prepare for technical interviews and ensure you have a deep understanding of system architecture and implementation.

---

## Table of Contents

1. [Architecture & Overview](#1-architecture--overview)
2. [FastAPI](#2-fastapi)
3. [LangChain / LangGraph](#3-langchain--langgraph)
4. [VectorDB & FAISS](#4-vectordb--faiss)
5. [PostgreSQL](#5-postgresql)
6. [Redis](#6-redis)
7. [WebSocket](#7-websocket)
8. [Integration & Best Practices](#8-integration--best-practices)
9. [Advanced & Scenario-Based Questions](#9-advanced--scenario-based-questions)
10. [Specialized & Advanced Topics](#10-specialized--advanced-topics)
11. [Stump-the-Candidate: Deep-Dive Interview Q&A](#11-stump-the-candidate-deep-dive-interview-qa)
12. [Languages, Databases, and Tools](#12-languages-databases-and-tools)
13. [Advanced Topics for Robust AI Agents](#13-advanced-topics-for-robust-ai-agents)
14. [End-to-End Example: Building a RAG Agent](#14-end-to-end-example-building-a-rag-agent)
15. [Testing & CI/CD for AI Agents](#15-testing--cicd-for-ai-agents)
16. [Frontend/UX Integration](#16-frontendux-integration)
17. [Deployment Templates & Cloud Tips](#17-deployment-templates--cloud-tips)
18. [LLM & Embedding Model Selection](#18-llm--embedding-model-selection)
19. [Data Privacy & Compliance Checklist](#19-data-privacy--compliance-checklist)
20. [Performance Benchmarks & Optimization](#20-performance-benchmarks--optimization)
21. [References & Further Reading](#21-references--further-reading)

---

## 1. Architecture & Overview

### Q1: Can you describe the overall architecture of your AI agent system?

**Answer:**
The AI agent system is designed as a modular, scalable backend service. It uses FastAPI as the web framework to expose REST and WebSocket endpoints. LangChain/LangGraph orchestrates the agent's reasoning and workflow, integrating with a VectorDB (backed by FAISS) for semantic search and retrieval. PostgreSQL is used for structured data storage, while Redis provides fast caching and pub/sub for real-time features. WebSocket enables real-time, bidirectional communication with clients. The system is containerized for deployment and can scale horizontally.

**Architecture Diagram (Mermaid):**

```mermaid
graph TD
    Client((Client))
    subgraph Backend
        FastAPI[FastAPI API Layer]
        LangChain[LangChain / LangGraph]
        VectorDB[VectorDB/FAISS]
        Postgres[PostgreSQL]
        Redis[Redis]
    end
    Client-->|HTTP/WebSocket|FastAPI
    FastAPI-->|Invoke|LangChain
    LangChain-->|Semantic Search|VectorDB
    LangChain-->|Structured Data|Postgres
    LangChain-->|Cache/PubSub|Redis
    FastAPI-->|WebSocket|Client

```

### Q2: What are the main components and their responsibilities?

**Answer:**

- **FastAPI:** API layer for HTTP/WebSocket communication with automatic OpenAPI docs.

- **LangChain/LangGraph:** Agent logic, workflow orchestration, tool use, and LLM integration.

- **VectorDB (FAISS):** Semantic search and retrieval of vectorized data.

- **PostgreSQL:** Persistent storage for structured data (users, logs, configs).

- **Redis:** Caching, session management, and pub/sub for real-time updates.

- **WebSocket:** Real-time communication channel for streaming responses.

### Q3: How do these components interact during a typical user query?

**Answer:**

1. User sends a query via FastAPI endpoint (HTTP or WebSocket).
2. FastAPI passes the query to the agent logic (LangChain/LangGraph).
3. The agent may retrieve context from VectorDB (FAISS) and/or PostgreSQL.
4. Redis is used for caching or real-time pub/sub if needed.
5. The agent processes the query, possibly invoking an LLM, and returns the response via FastAPI (HTTP/WebSocket).

**Example Code (FastAPI endpoint with LangChain):**

```python
from fastapi import FastAPI, WebSocket
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

app = FastAPI()
llm = ChatOpenAI(model="gpt-4o")

@app.post("/query")
async def query_agent(request: dict):
    user_input = request["input"]
    response = await llm.ainvoke([HumanMessage(content=user_input)])
    return {"response": response.content}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    while True:
        data = await websocket.receive_text()
        response = await llm.ainvoke([HumanMessage(content=data)])
        await websocket.send_text(response.content)

```

---

## 2. FastAPI

### Q4: Why did you choose FastAPI for this project?

**Answer:**
FastAPI offers high performance (on par with Node.js and Go), native async support, automatic OpenAPI/Swagger documentation, and easy integration with modern Python async libraries. It is well-suited for both REST and WebSocket APIs, making it ideal for AI agent backends that require low latency and high throughput.

### Q5: How do you handle request validation and serialization in FastAPI?

**Answer:**
FastAPI uses Pydantic v2 models for request validation and response serialization. This ensures type safety, automatic documentation, and clear API contracts.

**Example:**

```python
from pydantic import BaseModel, Field

class QueryRequest(BaseModel):
    input: str = Field(..., min_length=1, max_length=10000)
    session_id: str | None = None

class QueryResponse(BaseModel):
    response: str
    tokens_used: int | None = None

@app.post("/query", response_model=QueryResponse)
async def query_agent(request: QueryRequest):
    response = await llm.ainvoke([HumanMessage(content=request.input)])
    return QueryResponse(response=response.content, tokens_used=response.usage_metadata.get("total_tokens"))

```

### Q6: How do you implement authentication and authorization?

**Answer:**
Authentication can be implemented using OAuth2, JWT, or API keys. FastAPI provides dependency injection for security schemes. Authorization is enforced at the route or business logic level.

**Example (JWT Auth with python-jose):**

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import jwt, JWTError

security = HTTPBearer()
SECRET_KEY = "your-secret-key"
ALGORITHM = "HS256"

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    try:
        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[ALGORITHM])
        return payload.get("sub")
    except JWTError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

@app.post("/query")
async def query_agent(request: QueryRequest, user: str = Depends(get_current_user)):
    # user is authenticated
    ...

```

### Q7: How do you manage asynchronous operations in FastAPI?

**Answer:**
FastAPI natively supports async/await, allowing for non-blocking I/O operations (e.g., database, network calls), which improves scalability and performance. Use async database drivers (asyncpg, redis.asyncio) and async HTTP clients (httpx).

**Example:**

```python
import httpx

@app.get("/external-data")
async def get_external_data():
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.example.com/data")
    return response.json()

```

### Q8: How do you handle errors and exceptions globally in FastAPI?

**Answer:**
Use exception handlers to catch and format errors consistently across all endpoints.

**Example:**

```python
from fastapi import Request
from fastapi.responses import JSONResponse

class AgentException(Exception):
    def __init__(self, message: str, status_code: int = 500):
        self.message = message
        self.status_code = status_code

@app.exception_handler(AgentException)
async def agent_exception_handler(request: Request, exc: AgentException):
    return JSONResponse(status_code=exc.status_code, content={"error": exc.message})

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    return JSONResponse(status_code=500, content={"error": "Internal server error"})

```

---

## 3. LangChain / LangGraph

### Q9: What is LangChain/LangGraph and why did you use it?

**Answer:**
LangChain is a framework for building applications with LLMs, providing tools for chaining prompts, memory, tool use, and retrieval. LangGraph extends this with graph-based workflows for complex, stateful agent logic with cycles and branching. They simplify building multi-step agent workflows.

**Example (LangChain with tools - 2026 API):**

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

@tool
def search(query: str) -> str:
    """Search the web for information."""
    return f"Results for: {query}"

llm = ChatOpenAI(model="gpt-4o")
agent = create_react_agent(llm, tools=[search])

result = agent.invoke({"messages": [("user", "Search for LangChain docs")]})

```

### Q10: How do you define and manage agent workflows with LangGraph?

**Answer:**
LangGraph uses a graph-based approach where nodes are functions and edges define the flow. It supports cycles, conditionals, and human-in-the-loop patterns.

**Example (LangGraph workflow - 2026 API):**

```python
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

class State(TypedDict):
    messages: list
    context: str

def retrieve_context(state: State) -> State:
    # Retrieve from vector DB
    state["context"] = "Retrieved context here"
    return state

def generate_response(state: State) -> State:
    # Generate with LLM using context
    state["messages"].append({"role": "assistant", "content": f"Response with {state['context']}"})
    return state

workflow = StateGraph(State)
workflow.add_node("retrieve", retrieve_context)
workflow.add_node("generate", generate_response)
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

app = workflow.compile()
result = app.invoke({"messages": [], "context": ""})

```

### Q11: How do you integrate external tools or APIs with your agent?

**Answer:**
LangChain supports tool integration via the `@tool` decorator or `StructuredTool`. Tools can call APIs, databases, or perform computations.

**Example (structured tool with validation):**

```python
from langchain_core.tools import tool
from pydantic import BaseModel, Field

class CalculatorInput(BaseModel):
    expression: str = Field(description="Mathematical expression to evaluate")

@tool(args_schema=CalculatorInput)
def calculator(expression: str) -> str:
    """Evaluate a mathematical expression safely."""
    try:
        # Use ast.literal_eval for safety
        import ast
        result = eval(compile(ast.parse(expression, mode='eval'), '', 'eval'))
        return str(result)
    except Exception as e:
        return f"Error: {e}"

```

### Q12: How do you handle context and memory in the agent?

**Answer:**
LangGraph provides built-in checkpointing for conversation memory. You can persist state to databases for cross-session memory.

**Example (memory with checkpointing):**

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

checkpointer = MemorySaver()
agent = create_react_agent(llm, tools=[], checkpointer=checkpointer)

# Each thread_id maintains separate conversation history
config = {"configurable": {"thread_id": "user-123"}}
result = agent.invoke({"messages": [("user", "Hello")]}, config=config)
result = agent.invoke({"messages": [("user", "What did I just say?")]}, config=config)

```

### Q13: What are guardrails and how do you implement them?

**Answer:**
Guardrails are safety mechanisms to prevent harmful outputs, validate inputs, and ensure agent behavior stays within bounds. They include input validation, output filtering, and content moderation.

**Example:**

```python
from langchain_core.output_parsers import StrOutputParser

def input_guardrail(user_input: str) -> str:
    blocked_patterns = ["ignore previous instructions", "system prompt"]
    for pattern in blocked_patterns:
        if pattern.lower() in user_input.lower():
            raise ValueError("Input contains blocked content")
    return user_input

def output_guardrail(response: str) -> str:
    # Filter PII, harmful content, etc.
    import re
    # Remove potential SSN patterns
    response = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '[REDACTED]', response)
    return response

@app.post("/query")
async def safe_query(request: QueryRequest):
    validated_input = input_guardrail(request.input)
    response = await llm.ainvoke([HumanMessage(content=validated_input)])
    safe_response = output_guardrail(response.content)
    return {"response": safe_response}

```

### Q14: How do you get structured outputs from LLMs?

**Answer:**
Use Pydantic models with LangChain's `with_structured_output()` to ensure consistent, typed responses.

**Example:**

```python
from pydantic import BaseModel
from langchain_openai import ChatOpenAI

class ExtractedInfo(BaseModel):
    name: str
    email: str
    intent: str

llm = ChatOpenAI(model="gpt-4o")
structured_llm = llm.with_structured_output(ExtractedInfo)

result = structured_llm.invoke("My name is John, email john@example.com, I want to cancel my order")
print(result.name)  # "John"
print(result.email)  # "john@example.com"

```

---

## 4. VectorDB & FAISS

### Q15: What is a VectorDB and why is FAISS used?

**Answer:**
A VectorDB stores high-dimensional vector embeddings for semantic search. FAISS (Facebook AI Similarity Search) is a fast, efficient library for similarity search and clustering of dense vectors, ideal for RAG pipelines. For production, consider managed solutions like Pinecone, Weaviate, Qdrant, or pgvector.

**Diagram:**

```text
User Query -> [Embedder] -> [VectorDB/FAISS] <-> [Document Vectors]
```

### Q16: How do you generate and store embeddings?

**Answer:**
Embeddings are generated using embedding models (OpenAI, HuggingFace, Cohere). The vectors are stored in FAISS or a vector database, indexed for fast similarity search.

**Example (2026 best practices):**

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Create from documents
texts = ["AI is transforming industries", "LangChain simplifies LLM apps"]
vectorstore = FAISS.from_texts(texts, embeddings)

# Save and load
vectorstore.save_local("faiss_index")
loaded_vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

```

### Q17: How do you perform semantic search in your system?

**Answer:**
A user query is embedded into a vector, and FAISS finds the most similar vectors/documents. Results are used as context for the agent.

**Example:**

```python
# Similarity search
query = "What is LangChain?"
docs = vectorstore.similarity_search(query, k=3)

# With relevance scores
docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)
for doc, score in docs_with_scores:
    print(f"Score: {score}, Content: {doc.page_content}")

```

### Q18: What are different FAISS index types and when to use them?

**Answer:**

- **IndexFlatL2/IP:** Exact search, best for small datasets (<100k vectors)

- **IndexIVFFlat:** Approximate search with clustering, good for medium datasets

- **IndexIVFPQ:** Product quantization for large datasets, trades accuracy for speed/memory

- **IndexHNSW:** Graph-based, excellent recall with fast search

**Example:**

```python
import faiss
import numpy as np

d = 384  # dimension
nlist = 100  # number of clusters

# IVF index for larger datasets
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFFlat(quantizer, d, nlist)

# Train on sample data
training_data = np.random.rand(10000, d).astype('float32')
index.train(training_data)
index.add(training_data)

# Search with nprobe parameter
index.nprobe = 10  # search 10 nearest clusters
D, I = index.search(query_vec, k=5)

```

### Q19: How do you implement hybrid search (keyword + vector)?

**Answer:**
Hybrid search combines BM25/keyword search with vector similarity for better retrieval. Use ensemble retrievers or databases with hybrid support (pgvector + full-text).

**Example:**

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# BM25 for keyword search
bm25_retriever = BM25Retriever.from_texts(texts)
bm25_retriever.k = 3

# Vector retriever
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Ensemble with reciprocal rank fusion
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]
)

docs = ensemble_retriever.invoke("What is RAG?")

```

---

## 5. PostgreSQL

### Q20: What role does PostgreSQL play in your system?

**Answer:**
PostgreSQL is used for structured, relational data storage (users, logs, configs, metadata). It provides ACID guarantees, supports complex queries, and with pgvector extension, can also serve as a vector database.

### Q21: How do you interact with PostgreSQL from FastAPI?

**Answer:**
Use SQLAlchemy 2.0 with asyncpg for efficient, non-blocking database access.

**Example (SQLAlchemy 2.0 async):**

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy import Column, Integer, String, select

class Base(DeclarativeBase):
    pass

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True)
    name = Column(String)

engine = create_async_engine("postgresql+asyncpg://user:pass@host/db")
async_session = async_sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

async def get_user(user_id: int):
    async with async_session() as session:
        result = await session.execute(select(User).where(User.id == user_id))
        return result.scalar_one_or_none()

```

### Q22: How do you use pgvector for vector search in PostgreSQL?

**Answer:**
pgvector adds vector similarity search to PostgreSQL, enabling hybrid queries combining relational filters with semantic search.

**Example:**

```sql
-- Enable extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create table with vector column
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    metadata JSONB,
    embedding vector(1536)
);

-- Create index for fast search
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- Hybrid search: filter + vector similarity
SELECT id, content, 1 - (embedding <=> '[0.1, 0.2, ...]') as similarity
FROM documents
WHERE metadata->>'category' = 'tech'
ORDER BY embedding <=> '[0.1, 0.2, ...]'
LIMIT 5;

```

### Q23: How do you handle migrations and schema changes?

**Answer:**
Use Alembic for database migrations with version control and safe rollbacks.

**Example:**

```bash
# Initialize Alembic
alembic init alembic

# Generate migration
alembic revision --autogenerate -m "add documents table"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1

```

---

## 6. Redis

### Q24: What is Redis used for in your architecture?

**Answer:**
Redis is used for caching (LLM responses, embeddings), session management, rate limiting, pub/sub for real-time features, and as a task queue broker.

### Q25: How do you use Redis for caching with modern async API?

**Answer:**
Use redis-py with asyncio support for non-blocking cache operations.

**Example (redis-py async - 2026):**

```python
import redis.asyncio as redis
import json

redis_client = redis.from_url("redis://localhost:6379")

async def get_cached_response(query: str) -> str | None:
    cached = await redis_client.get(f"query:{query}")
    return cached.decode() if cached else None

async def cache_response(query: str, response: str, ttl: int = 3600):
    await redis_client.setex(f"query:{query}", ttl, response)

# Semantic caching with embeddings
async def get_semantic_cache(query_embedding: list[float], threshold: float = 0.95):
    # Store embeddings and use vector similarity for cache hits
    # Consider using Redis Stack with vector search
    pass

```

### Q26: How do you implement rate limiting with Redis?

**Answer:**
Use Redis for distributed rate limiting with sliding window or token bucket algorithms.

**Example:**

```python
import redis.asyncio as redis
from fastapi import HTTPException

async def rate_limit(user_id: str, limit: int = 100, window: int = 60):
    key = f"rate:{user_id}"
    pipe = redis_client.pipeline()
    pipe.incr(key)
    pipe.expire(key, window)
    results = await pipe.execute()
    count = results[0]
    
    if count > limit:
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    return count

```

### Q27: How do you use Redis pub/sub for real-time features?

**Answer:**
Redis pub/sub broadcasts real-time updates (agent progress, notifications) to subscribed clients.

**Example:**

```python
import redis.asyncio as redis
import asyncio

async def publish_progress(session_id: str, message: str):
    await redis_client.publish(f"progress:{session_id}", message)

async def subscribe_progress(session_id: str):
    pubsub = redis_client.pubsub()
    await pubsub.subscribe(f"progress:{session_id}")
    async for message in pubsub.listen():
        if message["type"] == "message":
            yield message["data"].decode()

```

---

## 7. WebSocket

### Q28: Why did you use WebSocket in your system?

**Answer:**
WebSocket enables real-time, bidirectional communication, allowing the agent to stream responses token-by-token, send progress updates, and handle long-running conversations efficiently.

### Q29: How do you implement streaming responses with WebSocket?

**Answer:**
Stream LLM tokens as they're generated for better UX.

**Example:**

```python
from fastapi import WebSocket, WebSocketDisconnect
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o", streaming=True)

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_text()
            async for chunk in llm.astream([HumanMessage(content=data)]):
                await websocket.send_text(chunk.content)
            await websocket.send_text("[DONE]")
    except WebSocketDisconnect:
        pass

```

### Q30: How do you handle WebSocket authentication?

**Answer:**
Authenticate during the initial handshake using tokens in query params or the first message.

**Example:**

```python
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    token = websocket.query_params.get("token")
    try:
        user = await verify_token(token)
    except Exception:
        await websocket.close(code=4001, reason="Unauthorized")
        return
    
    await websocket.accept()
    # Connection authenticated, proceed...

```

### Q31: How do you scale WebSocket connections across multiple servers?

**Answer:**
Use Redis pub/sub or a message broker to broadcast messages across all server instances.

**Example:**

```python
class ConnectionManager:
    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}
        self.redis = redis.from_url("redis://localhost:6379")
    
    async def connect(self, user_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[user_id] = websocket
    
    async def broadcast_via_redis(self, channel: str, message: str):
        await self.redis.publish(channel, message)
    
    async def listen_redis(self):
        pubsub = self.redis.pubsub()
        await pubsub.subscribe("broadcast")
        async for msg in pubsub.listen():
            if msg["type"] == "message":
                for ws in self.active_connections.values():
                    await ws.send_text(msg["data"].decode())

```

---

## 8. Integration & Best Practices

### Q32: How do you orchestrate interactions between all components?

**Answer:**
The agent logic (LangGraph) acts as the orchestrator, invoking VectorDB, PostgreSQL, and Redis as needed. FastAPI exposes the API, and WebSocket provides real-time communication. Each component is loosely coupled via dependency injection.

**Example:**

```python
from fastapi import Depends
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: initialize connections
    app.state.redis = redis.from_url("redis://localhost:6379")
    app.state.vectorstore = FAISS.load_local("index", embeddings)
    yield
    # Shutdown: cleanup
    await app.state.redis.close()

app = FastAPI(lifespan=lifespan)

async def get_dependencies(request: Request):
    return {
        "redis": request.app.state.redis,
        "vectorstore": request.app.state.vectorstore,
    }

@app.post("/query")
async def query(request: QueryRequest, deps=Depends(get_dependencies)):
    # Use deps["redis"], deps["vectorstore"], etc.
    pass

```

### Q33: How do you ensure scalability and performance?

**Answer:**

- Use async I/O throughout (FastAPI, asyncpg, redis.asyncio)
- Cache frequent queries and embeddings in Redis
- Use efficient vector indices (HNSW, IVF)
- Scale horizontally with Kubernetes
- Profile and optimize hot paths
- Use connection pooling for databases

### Q34: How do you monitor and log your system?

**Answer:**
Use structured logging (JSON), distributed tracing (OpenTelemetry), and metrics (Prometheus/Grafana).

**Example:**

```python
import structlog
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Structured logging
structlog.configure(processors=[structlog.processors.JSONRenderer()])
logger = structlog.get_logger()

# Tracing
FastAPIInstrumentor.instrument_app(app)

@app.post("/query")
async def query(request: QueryRequest):
    tracer = trace.get_tracer(__name__)
    with tracer.start_as_current_span("process_query") as span:
        span.set_attribute("query.length", len(request.input))
        logger.info("processing_query", query=request.input[:100])
        # ... process

```

### Q35: How do you deploy your system?

**Answer:**
Containerize with Docker, use CI/CD pipelines, and deploy with Kubernetes for orchestration.

**Example (Dockerfile - Python 3.12):**

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Run with uvicorn
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]

```

### Q36: How do you handle versioning and backward compatibility?

**Answer:**
Version your API endpoints, use semantic versioning, and maintain backward compatibility.

**Example:**

```python
from fastapi import APIRouter

v1_router = APIRouter(prefix="/api/v1")
v2_router = APIRouter(prefix="/api/v2")

@v1_router.post("/query")
async def query_v1(request: QueryRequestV1):
    # Legacy format
    pass

@v2_router.post("/query")
async def query_v2(request: QueryRequestV2):
    # New format with additional fields
    pass

app.include_router(v1_router)
app.include_router(v2_router)

```

---

## 9. Advanced & Scenario-Based Questions

### Q37: How would you add support for a new LLM or embedding model?

**Answer:**
Abstract the LLM/embedding interfaces and implement adapters for new providers.

**Example:**

```python
from abc import ABC, abstractmethod
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_anthropic import ChatAnthropic

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str) -> str:
        pass

class OpenAIProvider(LLMProvider):
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o")
    
    async def generate(self, prompt: str) -> str:
        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        return response.content

class AnthropicProvider(LLMProvider):
    def __init__(self):
        self.llm = ChatAnthropic(model="claude-sonnet-4-20250514")
    
    async def generate(self, prompt: str) -> str:
        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        return response.content

# Factory pattern
def get_llm_provider(provider_name: str) -> LLMProvider:
    providers = {"openai": OpenAIProvider, "anthropic": AnthropicProvider}
    return providers[provider_name]()

```

### Q38: How would you handle a sudden spike in user traffic?

**Answer:**

- Horizontal scaling with Kubernetes autoscaler
- Queue requests with Redis/Celery for backpressure
- Aggressive caching (semantic cache for similar queries)
- Rate limiting per user/tier
- Circuit breakers for external services

### Q39: How would you debug a slow semantic search?

**Answer:**

1. Profile embedding generation time
2. Check FAISS index type (use IVF/HNSW for large datasets)
3. Monitor GPU/CPU utilization
4. Check for network latency to vector DB
5. Consider approximate nearest neighbors with tuned parameters

### Q40: How would you migrate from FAISS to another VectorDB?

**Answer:**

1. Abstract the vector store interface
2. Implement new backend (Pinecone, Weaviate, etc.)
3. Dual-write during migration
4. Verify with similarity tests
5. Switch traffic gradually

### Q41: How would you implement multi-tenancy in your system?

**Answer:**

- Add tenant_id to all data models
- Separate vector indices per tenant or filter by tenant_id
- Row-level security in PostgreSQL
- Tenant-aware rate limiting

**Example:**

```python
class Document(Base):
    id = Column(Integer, primary_key=True)
    tenant_id = Column(String, index=True)
    content = Column(Text)
    embedding = Column(Vector(1536))

# Query with tenant filter
async def search_documents(tenant_id: str, query_embedding: list):
    result = await session.execute(
        select(Document)
        .where(Document.tenant_id == tenant_id)
        .order_by(Document.embedding.cosine_distance(query_embedding))
        .limit(5)
    )
    return result.scalars().all()

```

### Q42: How would you ensure data privacy and compliance?

**Answer:**

- Encrypt data at rest (database encryption) and in transit (TLS)
- Implement data retention policies with automatic deletion
- Audit logging for compliance
- PII detection and redaction
- Data residency controls

### Q43: How would you add a new tool or plugin to the agent?

**Answer:**
Define the tool with the `@tool` decorator and add to the agent's toolset.

**Example:**

```python
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

@tool
def get_weather(city: str) -> str:
    """Get current weather for a city."""
    # Call weather API
    return f"Weather in {city}: Sunny, 72Â°F"

@tool
def search_database(query: str) -> str:
    """Search the internal database."""
    # Query PostgreSQL
    return "Database results..."

# Add tools to agent
agent = create_react_agent(llm, tools=[get_weather, search_database])

```

### Q44: How would you handle long-running tasks or streaming responses?

**Answer:**

- WebSocket for streaming tokens
- Background tasks with Celery/ARQ for long operations
- Server-Sent Events (SSE) for one-way streaming

**Example (SSE):**

```python
from fastapi.responses import StreamingResponse

@app.post("/stream")
async def stream_response(request: QueryRequest):
    async def generate():
        async for chunk in llm.astream([HumanMessage(content=request.input)]):
            yield f"data: {chunk.content}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")

```

### Q45: How would you persist chat history or agent state?

**Answer:**
Use LangGraph checkpointers with database backends for persistence.

**Example (PostgreSQL checkpointer):**

```python
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

async def create_agent_with_persistence():
    conn_string = "postgresql://user:pass@localhost/db"
    checkpointer = AsyncPostgresSaver.from_conn_string(conn_string)
    await checkpointer.setup()
    
    agent = create_react_agent(llm, tools=[], checkpointer=checkpointer)
    
    # Each thread maintains its own history
    config = {"configurable": {"thread_id": "session-123"}}
    result = await agent.ainvoke({"messages": [("user", "Hello")]}, config=config)
    return result

```

### Q46: How would you implement rate limiting?

**Answer:**
Use Redis with sliding window algorithm and FastAPI middleware.

**Example:**

```python
from fastapi import Depends, HTTPException
import redis.asyncio as redis
import time

async def rate_limiter(
    user_id: str,
    limit: int = 60,
    window: int = 60
) -> bool:
    now = time.time()
    key = f"ratelimit:{user_id}"
    
    pipe = redis_client.pipeline()
    pipe.zremrangebyscore(key, 0, now - window)
    pipe.zadd(key, {str(now): now})
    pipe.zcard(key)
    pipe.expire(key, window)
    results = await pipe.execute()
    
    request_count = results[2]
    if request_count > limit:
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    return True

```

### Q47: How would you expose metrics and health checks?

**Answer:**
Add dedicated endpoints for health checks and Prometheus metrics.

**Example:**

```python
from prometheus_client import Counter, Histogram, generate_latest
from fastapi.responses import PlainTextResponse

REQUEST_COUNT = Counter("requests_total", "Total requests", ["endpoint", "status"])
REQUEST_LATENCY = Histogram("request_latency_seconds", "Request latency")

@app.get("/health")
async def health_check():
    # Check dependencies
    try:
        await redis_client.ping()
        async with async_session() as session:
            await session.execute(text("SELECT 1"))
        return {"status": "healthy"}
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

@app.get("/metrics")
async def metrics():
    return PlainTextResponse(generate_latest(), media_type="text/plain")

```

### Q48: How would you support multiple languages or locales?

**Answer:**

- Use multilingual embedding models (e.g., multilingual-e5-large)
- Detect language and route to appropriate models
- Store locale metadata for filtering

**Example:**

```python
from langdetect import detect

@tool
def translate_and_respond(query: str, target_lang: str = "en") -> str:
    """Detect language and respond appropriately."""
    detected_lang = detect(query)
    # Use multilingual model or translate
    return f"Detected: {detected_lang}, responding in {target_lang}"

```

---

## 10. Specialized & Advanced Topics

### Q49: What is BERT and how can you use it in your agent?

**Answer:**
BERT (Bidirectional Encoder Representations from Transformers) is a transformer model for NLU. Use it for embeddings, classification, or question answering.

**Example (HuggingFace):**

```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

def get_embedding(text: str) -> list[float]:
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Mean pooling
    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
    return embedding

```

### Q50: How do you evaluate the performance of your AI agent?

**Answer:**
Use task-specific metrics and evaluation frameworks like RAGAS for RAG systems.

**Example (RAGAS evaluation):**

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

# Prepare evaluation dataset
dataset = {
    "question": ["What is LangChain?"],
    "answer": ["LangChain is a framework for LLM apps"],
    "contexts": [["LangChain is a framework for building LLM applications"]],
    "ground_truth": ["LangChain is a framework for building LLM applications"]
}

results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision]
)
print(results)

```

### Q51: What is Model Context Protocol (MCP) and how is it used?

**Answer:**
MCP is an open standard by Anthropic for connecting AI assistants to external data sources and tools. It provides a standardized way to expose resources, tools, and prompts to LLMs.

**Example (MCP server):**

```python
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent

server = Server("my-tools")

@server.list_tools()
async def list_tools():
    return [
        Tool(
            name="search_docs",
            description="Search documentation",
            inputSchema={"type": "object", "properties": {"query": {"type": "string"}}}
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict):
    if name == "search_docs":
        results = await search_vectorstore(arguments["query"])
        return [TextContent(type="text", text=str(results))]

async def main():
    async with stdio_server() as (read, write):
        await server.run(read, write)

```

### Q52: How do you process audio files in your agent?

**Answer:**
Use speech-to-text (Whisper) for transcription and text-to-speech for responses.

**Example:**

```python
from openai import OpenAI
import base64

client = OpenAI()

async def transcribe_audio(audio_bytes: bytes) -> str:
    # Using Whisper API
    response = client.audio.transcriptions.create(
        model="whisper-1",
        file=("audio.webm", audio_bytes)
    )
    return response.text

async def text_to_speech(text: str) -> bytes:
    response = client.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=text
    )
    return response.content

```

### Q53: How do you implement agentic patterns like planning and reflection?

**Answer:**
Use LangGraph to build agents that can plan steps, execute them, and reflect on results.

**Example (ReAct with reflection):**

```python
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, Literal

class AgentState(TypedDict):
    task: str
    plan: list[str]
    current_step: int
    results: list[str]
    reflection: str
    final_answer: str

def planner(state: AgentState) -> AgentState:
    # LLM generates a plan
    plan = llm.invoke(f"Create a step-by-step plan for: {state['task']}")
    state["plan"] = plan.content.split("\n")
    return state

def executor(state: AgentState) -> AgentState:
    # Execute current step
    step = state["plan"][state["current_step"]]
    result = llm.invoke(f"Execute this step: {step}")
    state["results"].append(result.content)
    state["current_step"] += 1
    return state

def reflector(state: AgentState) -> AgentState:
    # Reflect on progress
    reflection = llm.invoke(f"Reflect on progress: {state['results']}")
    state["reflection"] = reflection.content
    return state

def should_continue(state: AgentState) -> Literal["executor", "synthesizer"]:
    if state["current_step"] < len(state["plan"]):
        return "executor"
    return "synthesizer"

workflow = StateGraph(AgentState)
workflow.add_node("planner", planner)
workflow.add_node("executor", executor)
workflow.add_node("reflector", reflector)
workflow.add_edge(START, "planner")
workflow.add_edge("planner", "executor")
workflow.add_conditional_edges("executor", should_continue)

```

---

## 11. Stump-the-Candidate: Deep-Dive Interview Q&A

### Q54: How would you handle cross-service failures or partial outages?

**Answer:**

- Implement retries with exponential backoff
- Use circuit breakers (e.g., circuitbreaker library)
- Gracefully degrade features (fallback to cached data)
- Health checks and alerting

**Example:**

```python
from circuitbreaker import circuit
import tenacity

@tenacity.retry(
    stop=tenacity.stop_after_attempt(3),
    wait=tenacity.wait_exponential(multiplier=1, min=2, max=10)
)
@circuit(failure_threshold=5, recovery_timeout=30)
async def call_external_api(query: str):
    async with httpx.AsyncClient() as client:
        response = await client.get(f"https://api.example.com/search?q={query}")
        response.raise_for_status()
        return response.json()

```

### Q55: What are the trade-offs of microservices vs. monolith for this agent?

**Answer:**

- **Microservices:** Independent scaling, fault isolation, polyglot friendly. Cons: network complexity, distributed tracing needed.

- **Monolith:** Simpler deployment, lower latency, easier debugging. Cons: scaling constraints, larger blast radius.
- Consider starting monolithic and extracting services as needed.

### Q56: How do you debug or trace errors in complex agent workflows?

**Answer:**

- Use LangSmith for LangChain/LangGraph tracing
- OpenTelemetry for distributed tracing
- Structured logging with request IDs
- Step-by-step workflow visualization

**Example:**

```python
# Enable LangSmith tracing
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-agent"

# All LangChain operations are now traced automatically

```

### Q57: How would you implement custom memory modules in LangGraph?

**Answer:**
Implement custom checkpointers for specialized memory needs.

**Example:**

```python
from langgraph.checkpoint.base import BaseCheckpointSaver
from typing import Any

class CustomMemory(BaseCheckpointSaver):
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def aget(self, config: dict) -> dict | None:
        key = f"checkpoint:{config['configurable']['thread_id']}"
        data = await self.redis.get(key)
        return json.loads(data) if data else None
    
    async def aput(self, config: dict, checkpoint: dict) -> dict:
        key = f"checkpoint:{config['configurable']['thread_id']}"
        await self.redis.set(key, json.dumps(checkpoint))
        return config

```

### Q58: How do you handle vector drift or embedding model upgrades?

**Answer:**

- Version embeddings with model identifier
- Re-embed documents in background when upgrading
- Keep old and new indices during transition
- A/B test retrieval quality

### Q59: What are the limitations of FAISS and how do you overcome them?

**Answer:**

- **Single-node:** Use distributed wrappers or managed services (Pinecone, Weaviate)

- **No metadata filtering:** Combine with PostgreSQL for hybrid queries

- **Memory-bound:** Use disk-based indices or quantization

### Q60: How do you securely rotate secrets in production without downtime?

**Answer:**

- Use secret managers with versioning (AWS Secrets Manager, HashiCorp Vault)
- Application reloads secrets on change (watch for updates)
- Dual-version support during transition

### Q61: How do you handle scaling WebSocket connections?

**Answer:**

- Use Redis pub/sub for cross-server communication
- Sticky sessions with load balancer
- Connection pooling and limits

### Q62: What are the security risks of WebSockets?

**Answer:**

- Lack of built-in authentication (validate on connect)
- Message injection (validate all messages)
- DoS (set message size limits, rate limiting)
- Use WSS (TLS) always

### Q63: How do you estimate and monitor LLM API costs?

**Answer:**

- Track token usage per request
- Log costs in structured logs
- Set budget alerts
- Cache expensive operations

**Example:**

```python
from langchain_core.callbacks import BaseCallbackHandler

class CostTracker(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        cost = (usage.get("prompt_tokens", 0) * 0.00001 + 
                usage.get("completion_tokens", 0) * 0.00003)
        logger.info("llm_cost", tokens=usage, cost_usd=cost)

```

### Q64: How do you measure and mitigate bias in agent outputs?

**Answer:**

- Use bias evaluation datasets
- Regular audits of outputs
- Prompt engineering to reduce bias
- User feedback mechanisms
- Content filtering

### Q65: How would you provide real-time explanations for LLM decisions?

**Answer:**

- Return retrieved context with responses
- Use chain-of-thought prompting
- Log intermediate reasoning steps

### Q66: How do you debug intermittent production bugs?

**Answer:**

- Detailed structured logging with correlation IDs
- Distributed tracing
- Error sampling and aggregation
- Reproduction environments with production data snapshots

### Q67: How do you trace memory leaks in async Python code?

**Answer:**

- Use tracemalloc for allocation tracking
- objgraph for reference analysis
- Check for unawaited coroutines
- Monitor event loop metrics

**Example:**

```python
import tracemalloc

tracemalloc.start()

# ... run code ...

snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')
for stat in top_stats[:10]:
    print(stat)

```

### Q68: What are cold start implications for serverless agents?

**Answer:**

- Cold starts add 1-5 seconds latency
- Mitigate with provisioned concurrency
- Minimize package size
- Use lazy loading for heavy dependencies

### Q69: How do you manage stateful workflows in serverless?

**Answer:**

- Store state in DynamoDB/Redis
- Use Step Functions for orchestration
- Checkpoint after each step
- Idempotent operations

---

## 12. Languages, Databases, and Tools

### Q70: Why is Python the preferred language for AI agents?

**Answer:**

- Extensive AI/ML ecosystem (PyTorch, TensorFlow, HuggingFace)
- LangChain/LangGraph native support
- Rich data processing libraries (NumPy, pandas)
- Async support for high concurrency
- Rapid prototyping and readability

### Q71: What makes PostgreSQL a good choice for AI agent backends?

**Answer:**

- ACID compliance for data integrity
- pgvector extension for vector search
- JSONB for flexible schemas
- Full-text search capabilities
- Mature ecosystem and reliability

### Q72: Why use FAISS or a Vector Database?

**Answer:**

- Fast similarity search over millions of vectors
- Multiple index types for different scale/accuracy trade-offs
- CPU and GPU support
- Well-documented and battle-tested

### Q73: What other tools are essential for AI agents?

**Answer:**

- **LangChain/LangGraph:** Workflow orchestration

- **FastAPI:** Async API layer

- **Redis:** Caching and pub/sub

- **Celery/ARQ:** Background tasks

- **Prometheus/Grafana:** Monitoring

- **Docker/Kubernetes:** Deployment

---

## 13. Advanced Topics for Robust AI Agents

### Q74: What is prompt engineering and how does it impact performance?

**Answer:**
Prompt engineering designs inputs to guide LLM behavior. Good prompts improve accuracy, reduce hallucinations, and enable complex tasks.

**Example templates:**

**System prompt:**

```python
system_prompt = """You are a helpful AI assistant. Follow these rules:
1. Be concise and accurate
2. Cite sources when available
3. If unsure, say "I don't know"
4. Never make up information
"""

```

**Few-shot:**

```python
few_shot_prompt = """
Examples:
Q: What is FAISS?
A: FAISS is a library for efficient similarity search of dense vectors.

Q: What is RAG?
A: RAG (Retrieval-Augmented Generation) combines retrieval with generation for grounded responses.

Q: {question}
A:
"""

```

### Q75: How do you fine-tune LLMs for custom tasks?

**Answer:**

- Collect domain-specific data
- Use OpenAI fine-tuning API or HuggingFace Trainer
- Evaluate on held-out test sets
- Consider parameter-efficient methods (LoRA, QLoRA)

**Example (OpenAI fine-tuning):**

```python
from openai import OpenAI
client = OpenAI()

# Upload training file
file = client.files.create(
    file=open("training_data.jsonl", "rb"),
    purpose="fine-tune"
)

# Create fine-tuning job
job = client.fine_tuning.jobs.create(
    training_file=file.id,
    model="gpt-4o-mini-2024-07-18"
)

```

### Q76: What are advanced RAG patterns?

**Answer:**

- **Multi-hop retrieval:** Chain queries for complex reasoning

- **Reranking:** Use cross-encoders to reorder results

- **Hybrid search:** Combine keyword and vector search

- **Query expansion:** Augment queries with related terms

- **Contextual compression:** Summarize retrieved docs

**Example (reranking):**

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank

reranker = CohereRerank(model="rerank-v3.5")
compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 20})
)

# Retrieves 20, reranks to top 5
docs = compression_retriever.invoke("What is LangChain?")

```

### Q77: How do you design multi-agent systems?

**Answer:**
Multiple specialized agents collaborate on complex tasks. Use LangGraph for orchestration.

**Example:**

```python
from langgraph.graph import StateGraph

# Define specialized agents
researcher_agent = create_react_agent(llm, tools=[web_search])
writer_agent = create_react_agent(llm, tools=[])
reviewer_agent = create_react_agent(llm, tools=[])

# Orchestration graph
workflow = StateGraph(State)
workflow.add_node("research", researcher_agent)
workflow.add_node("write", writer_agent)
workflow.add_node("review", reviewer_agent)
workflow.add_edge("research", "write")
workflow.add_edge("write", "review")

```

### Q78: How do you prevent prompt injection attacks?

**Answer:**

- Input sanitization and validation
- Separate user input from instructions
- Use structured outputs
- Monitor for suspicious patterns

**Example:**

```python
import re

INJECTION_PATTERNS = [
    r"ignore (all )?previous instructions",
    r"forget (your|the) instructions",
    r"new instructions:",
    r"system prompt:",
]

def detect_injection(user_input: str) -> bool:
    for pattern in INJECTION_PATTERNS:
        if re.search(pattern, user_input, re.IGNORECASE):
            return True
    return False

@app.post("/query")
async def safe_query(request: QueryRequest):
    if detect_injection(request.input):
        raise HTTPException(status_code=400, detail="Invalid input detected")
    # Process safely...

```

### Q79: How do you implement semantic caching?

**Answer:**
Cache responses based on semantic similarity, not just exact matches.

**Example:**

```python
class SemanticCache:
    def __init__(self, vectorstore, similarity_threshold: float = 0.95):
        self.vectorstore = vectorstore
        self.threshold = similarity_threshold
        self.cache = {}  # query_id -> response
    
    async def get(self, query: str) -> str | None:
        results = self.vectorstore.similarity_search_with_score(query, k=1)
        if results and results[0][1] >= self.threshold:
            cached_id = results[0][0].metadata["query_id"]
            return self.cache.get(cached_id)
        return None
    
    async def set(self, query: str, response: str):
        query_id = str(uuid.uuid4())
        self.vectorstore.add_texts([query], metadatas=[{"query_id": query_id}])
        self.cache[query_id] = response

```

### Q80: How do you implement user feedback loops?

**Answer:**
Collect feedback, analyze patterns, and use for model improvement.

**Example:**

```python
class Feedback(Base):
    __tablename__ = "feedback"
    id = Column(Integer, primary_key=True)
    query = Column(Text)
    response = Column(Text)
    rating = Column(Integer)  # 1-5
    feedback_text = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)

@app.post("/feedback")
async def submit_feedback(
    query: str,
    response: str,
    rating: int,
    feedback_text: str | None = None
):
    async with async_session() as session:
        feedback = Feedback(
            query=query,
            response=response,
            rating=rating,
            feedback_text=feedback_text
        )
        session.add(feedback)
        await session.commit()
    
    # Use low ratings for retraining dataset
    if rating <= 2:
        await add_to_retraining_queue(query, response, feedback_text)

```

---

## 14. End-to-End Example: Building a RAG Agent

### Project Structure

```text
rag-agent/
âââ app/
â   âââ __init__.py
â   âââ main.py
â   âââ agent.py
â   âââ vectorstore.py
â   âââ config.py
âââ tests/
â   âââ test_agent.py
âââ requirements.txt
âââ Dockerfile
âââ docker-compose.yml

```

### requirements.txt

```text
fastapi>=0.115.0
uvicorn>=0.32.0
langchain>=0.3.0
langchain-openai>=0.2.0
langgraph>=0.2.0
faiss-cpu>=1.9.0
sqlalchemy>=2.0.0
asyncpg>=0.30.0
redis>=5.0.0
pydantic>=2.0.0
python-jose>=3.3.0
httpx>=0.27.0

```

### app/main.py

```python
from fastapi import FastAPI, Depends, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager
import redis.asyncio as redis
from app.agent import create_agent
from app.vectorstore import load_vectorstore

@asynccontextmanager
async def lifespan(app: FastAPI):
    app.state.redis = redis.from_url("redis://localhost:6379")
    app.state.vectorstore = await load_vectorstore()
    app.state.agent = create_agent(app.state.vectorstore)
    yield
    await app.state.redis.close()

app = FastAPI(lifespan=lifespan)

class QueryRequest(BaseModel):
    input: str
    session_id: str | None = None

@app.post("/query")
async def query(request: QueryRequest):
    config = {"configurable": {"thread_id": request.session_id or "default"}}
    result = await app.state.agent.ainvoke(
        {"messages": [("user", request.input)]},
        config=config
    )
    return {"response": result["messages"][-1].content}

@app.get("/health")
async def health():
    return {"status": "healthy"}

```

### app/agent.py

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

def create_agent(vectorstore):
    llm = ChatOpenAI(model="gpt-4o")
    
    @tool
    def search_knowledge_base(query: str) -> str:
        """Search the knowledge base for relevant information."""
        docs = vectorstore.similarity_search(query, k=3)
        return "\n\n".join([doc.page_content for doc in docs])
    
    checkpointer = MemorySaver()
    agent = create_react_agent(
        llm,
        tools=[search_knowledge_base],
        checkpointer=checkpointer
    )
    return agent

```

### Running the App

```bash
uvicorn app.main:app --reload

```

### Example Query

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"input": "What is LangChain?", "session_id": "user-123"}'

```

---

## 15. Testing & CI/CD for AI Agents

### Automated Testing

**Types of Tests:**

- **Unit tests:** Test individual functions, prompt templates

- **Integration tests:** Test end-to-end flows with mocked LLMs

- **Evaluation tests:** Test retrieval quality and response accuracy

**Example (pytest with mocking):**

```python
import pytest
from unittest.mock import AsyncMock, patch

@pytest.fixture
def mock_llm():
    with patch("app.agent.ChatOpenAI") as mock:
        mock.return_value.ainvoke = AsyncMock(
            return_value=type("Response", (), {"content": "Mocked response"})()
        )
        yield mock

@pytest.mark.asyncio
async def test_query_endpoint(mock_llm, client):
    response = await client.post("/query", json={"input": "test"})
    assert response.status_code == 200
    assert "response" in response.json()

```

### CI/CD Setup

**GitHub Actions (.github/workflows/ci.yml):**

```yaml
name: CI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:7
        ports:
          - 6379:6379
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio httpx
      - name: Run tests
        run: pytest tests/ -v

```

---

## 16. Frontend/UX Integration

### React Chat Component

```jsx
import React, { useState, useEffect, useRef } from 'react';

function Chat() {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState([]);
  const [isStreaming, setIsStreaming] = useState(false);
  const wsRef = useRef(null);

  useEffect(() => {
    wsRef.current = new WebSocket("ws://localhost:8000/ws/chat");
    wsRef.current.onmessage = (event) => {
      if (event.data === "[DONE]") {
        setIsStreaming(false);
      } else {
        setMessages(prev => {
          const last = prev[prev.length - 1];
          if (last?.role === "assistant") {
            return [...prev.slice(0, -1), { ...last, content: last.content + event.data }];
          }
          return [...prev, { role: "assistant", content: event.data }];
        });
      }
    };
    return () => wsRef.current?.close();
  }, []);

  const sendMessage = () => {
    if (!input.trim()) return;
    setMessages(prev => [...prev, { role: "user", content: input }]);
    wsRef.current?.send(input);
    setIsStreaming(true);
    setInput("");
  };

  return (
    <div className="chat-container">
      <div className="messages">
        {messages.map((msg, i) => (
          <div key={i} className={`message ${msg.role}`}>
            {msg.content}
          </div>
        ))}
      </div>
      <input
        value={input}
        onChange={e => setInput(e.target.value)}
        onKeyPress={e => e.key === 'Enter' && sendMessage()}
        disabled={isStreaming}
      />
      <button onClick={sendMessage} disabled={isStreaming}>
        Send
      </button>
    </div>
  );
}

export default Chat;

```

---

## 17. Deployment Templates & Cloud Tips

### docker-compose.yml

```yaml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://user:pass@db:5432/ragdb
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
      - redis

  db:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: ragdb
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:

```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-agent
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-agent
  template:
    metadata:
      labels:
        app: ai-agent
    spec:
      containers:
      - name: agent
        image: myrepo/ai-agent:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-key

```

---

## 18. LLM & Embedding Model Selection

### Decision Matrix

| Use Case | OpenAI API | Azure OpenAI | Anthropic Claude | Local (Ollama) |
| ---------- | :----------: | :------------: | :----------------: | :--------------: |
| Best accuracy | â | â | â | â |
| Data privacy | â | â | â | â |
| Cost control | â | â | â | â |
| Enterprise compliance | â | â | â | â |
| Fastest setup | â | â | â | â |

### Embedding Models (2026)

| Model | Dimensions | Best For |
| ------- | ------------ | ---------- |
| text-embedding-3-small | 1536 | General purpose, cost-effective |
| text-embedding-3-large | 3072 | Maximum accuracy |
| multilingual-e5-large | 1024 | Multi-language support |
| bge-m3 | 1024 | Hybrid retrieval (dense + sparse) |

---

## 19. Data Privacy & Compliance Checklist

| Practice | Description |
| ---------- | ------------- |
| Minimize data collection | Only collect data needed for the use case |
| Anonymize/pseudonymize | Remove or mask user identifiers |
| User consent & deletion | GDPR/CCPA right to be forgotten |
| Encrypt at rest & transit | Use TLS 1.3, encrypt databases |
| Access controls | RBAC, least privilege |
| Audit logging | Log access to sensitive data |
| Secure secrets | Use secret managers, never commit credentials |
| PII detection | Scan and redact PII in logs and outputs |

---

## 20. Performance Benchmarks & Optimization

### Key Metrics

- **Latency (P50, P95, P99):** Response time distribution

- **Throughput:** Requests per second

- **Token usage:** Input/output tokens per request

- **Error rate:** Failed requests percentage

### Optimization Strategies

1. **Caching:** Redis for responses, semantic cache for similar queries
2. **Async I/O:** Use asyncio throughout
3. **Connection pooling:** Database and Redis pools
4. **Batching:** Batch embedding requests
5. **Model optimization:** Use smaller models for simple tasks
6. **Index optimization:** Choose appropriate FAISS index type

### Load Testing

```bash
# Using hey for HTTP load testing
hey -n 1000 -c 50 -m POST \
  -H "Content-Type: application/json" \
  -d '{"input": "test query"}' \
  http://localhost:8000/query

```

---

## 21. References & Further Reading

### Core Tools & Frameworks

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [LangChain Documentation](https://python.langchain.com/docs/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [FAISS Documentation](https://faiss.ai/)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Redis Documentation](https://redis.io/docs/)

### Tutorials & Examples

- [LangChain Templates](https://github.com/langchain-ai/langchain/tree/master/templates)
- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)
- [RAG Best Practices](https://www.anthropic.com/research/building-effective-agents)

### Cloud & Deployment

- [AWS Bedrock](https://aws.amazon.com/bedrock/)
- [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service)
- [Pinecone Vector DB](https://www.pinecone.io/)
- [Weaviate Vector DB](https://weaviate.io/)

### Responsible AI

- [OpenAI Safety Best Practices](https://platform.openai.com/docs/guides/safety-best-practices)
- [Anthropic Responsible AI](https://www.anthropic.com/research)
- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
